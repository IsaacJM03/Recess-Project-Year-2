{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95335f4a",
   "metadata": {},
   "source": [
    "# Eye Disease Dataset Analysis\n",
    "## Task 1: Data Exploration and Missing Values Analysis\n",
    "\n",
    "**Team Members:** Isaac + Jonathan  \n",
    "**Objective:** Explore dataset, identify missing values, and calculate percentage of missing data\n",
    "\n",
    "### Dataset Overview\n",
    "This dataset contains eye disease images categorized into:\n",
    "- Cataract\n",
    "- Diabetic Retinopathy  \n",
    "- Glaucoma\n",
    "- Normal (healthy eyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb25f47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a528b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "dataset_path = \"dataset\"\n",
    "\n",
    "# Get all category folders\n",
    "categories = [folder for folder in os.listdir(dataset_path) \n",
    "              if os.path.isdir(os.path.join(dataset_path, folder))]\n",
    "\n",
    "print(f\"Categories found: {categories}\")\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "\n",
    "# Count images in each category\n",
    "category_counts = {}\n",
    "total_images = 0\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    # Count image files (jpg, jpeg, png)\n",
    "    image_files = [f for f in os.listdir(category_path) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    category_counts[category] = len(image_files)\n",
    "    total_images += len(image_files)\n",
    "    print(f\"{category}: {len(image_files)} images\")\n",
    "\n",
    "print(f\"\\nTotal images in dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for dataset overview\n",
    "dataset_overview = pd.DataFrame({\n",
    "    'Category': list(category_counts.keys()),\n",
    "    'Image_Count': list(category_counts.values())\n",
    "})\n",
    "\n",
    "# Calculate percentages\n",
    "dataset_overview['Percentage'] = (dataset_overview['Image_Count'] / total_images * 100).round(2)\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(dataset_overview)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot\n",
    "ax1.bar(dataset_overview['Category'], dataset_overview['Image_Count'], color='skyblue', edgecolor='navy')\n",
    "ax1.set_title('Number of Images per Category')\n",
    "ax1.set_xlabel('Eye Disease Category')\n",
    "ax1.set_ylabel('Number of Images')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(dataset_overview['Image_Count']):\n",
    "    ax1.text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(dataset_overview['Image_Count'], labels=dataset_overview['Category'], autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Distribution of Images by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d51784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing/corrupted images and analyze image properties\n",
    "missing_data_analysis = {\n",
    "    'category': [],\n",
    "    'total_files': [],\n",
    "    'readable_images': [],\n",
    "    'corrupted_images': [],\n",
    "    'missing_percentage': [],\n",
    "    'avg_file_size_mb': [],\n",
    "    'min_file_size_mb': [],\n",
    "    'max_file_size_mb': []\n",
    "}\n",
    "\n",
    "print(\"Analyzing images for missing/corrupted data...\\n\")\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(category_path) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    readable_count = 0\n",
    "    corrupted_count = 0\n",
    "    file_sizes = []\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(category_path, image_file)\n",
    "        \n",
    "        try:\n",
    "            # Try to open and verify the image\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()  # Verify it's a valid image\n",
    "                readable_count += 1\n",
    "                \n",
    "                # Get file size in MB\n",
    "                file_size_mb = os.path.getsize(image_path) / (1024 * 1024)\n",
    "                file_sizes.append(file_size_mb)\n",
    "                \n",
    "        except Exception as e:\n",
    "            corrupted_count += 1\n",
    "            print(f\"Corrupted image found: {image_path} - Error: {str(e)}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_files = len(image_files)\n",
    "    missing_percentage = (corrupted_count / total_files * 100) if total_files > 0 else 0\n",
    "    \n",
    "    # Store results\n",
    "    missing_data_analysis['category'].append(category)\n",
    "    missing_data_analysis['total_files'].append(total_files)\n",
    "    missing_data_analysis['readable_images'].append(readable_count)\n",
    "    missing_data_analysis['corrupted_images'].append(corrupted_count)\n",
    "    missing_data_analysis['missing_percentage'].append(missing_percentage)\n",
    "    \n",
    "    if file_sizes:\n",
    "        missing_data_analysis['avg_file_size_mb'].append(np.mean(file_sizes))\n",
    "        missing_data_analysis['min_file_size_mb'].append(np.min(file_sizes))\n",
    "        missing_data_analysis['max_file_size_mb'].append(np.max(file_sizes))\n",
    "    else:\n",
    "        missing_data_analysis['avg_file_size_mb'].append(0)\n",
    "        missing_data_analysis['min_file_size_mb'].append(0)\n",
    "        missing_data_analysis['max_file_size_mb'].append(0)\n",
    "    \n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  - Total files: {total_files}\")\n",
    "    print(f\"  - Readable images: {readable_count}\")\n",
    "    print(f\"  - Corrupted images: {corrupted_count}\")\n",
    "    print(f\"  - Missing/Corrupted percentage: {missing_percentage:.2f}%\")\n",
    "    if file_sizes:\n",
    "        print(f\"  - Average file size: {np.mean(file_sizes):.3f} MB\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame for missing data analysis\n",
    "missing_data_df = pd.DataFrame(missing_data_analysis)\n",
    "print(\"Missing Data Analysis Summary:\")\n",
    "print(missing_data_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae698e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image dimensions\n",
    "print(\"Analyzing image dimensions...\\n\")\n",
    "\n",
    "dimension_analysis = {\n",
    "    'category': [],\n",
    "    'avg_width': [],\n",
    "    'avg_height': [],\n",
    "    'min_width': [],\n",
    "    'max_width': [],\n",
    "    'min_height': [],\n",
    "    'max_height': [],\n",
    "    'unique_dimensions': []\n",
    "}\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    image_files = [f for f in os.listdir(category_path) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    widths = []\n",
    "    heights = []\n",
    "    dimensions = set()\n",
    "    \n",
    "    # Sample first 50 images for dimension analysis (to speed up)\n",
    "    sample_files = image_files[:50] if len(image_files) > 50 else image_files\n",
    "    \n",
    "    for image_file in sample_files:\n",
    "        image_path = os.path.join(category_path, image_file)\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "                dimensions.add((width, height))\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if widths and heights:\n",
    "        dimension_analysis['category'].append(category)\n",
    "        dimension_analysis['avg_width'].append(np.mean(widths))\n",
    "        dimension_analysis['avg_height'].append(np.mean(heights))\n",
    "        dimension_analysis['min_width'].append(np.min(widths))\n",
    "        dimension_analysis['max_width'].append(np.max(widths))\n",
    "        dimension_analysis['min_height'].append(np.min(heights))\n",
    "        dimension_analysis['max_height'].append(np.max(heights))\n",
    "        dimension_analysis['unique_dimensions'].append(len(dimensions))\n",
    "        \n",
    "        print(f\"{category} (sample of {len(sample_files)} images):\")\n",
    "        print(f\"  - Average dimensions: {np.mean(widths):.0f} x {np.mean(heights):.0f}\")\n",
    "        print(f\"  - Width range: {np.min(widths)} - {np.max(widths)}\")\n",
    "        print(f\"  - Height range: {np.min(heights)} - {np.max(heights)}\")\n",
    "        print(f\"  - Unique dimensions: {len(dimensions)}\")\n",
    "        print()\n",
    "\n",
    "# Create dimension analysis DataFrame\n",
    "dimension_df = pd.DataFrame(dimension_analysis)\n",
    "print(\"Dimension Analysis Summary:\")\n",
    "print(dimension_df.round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Missing/Corrupted data percentage\n",
    "ax1.bar(missing_data_df['category'], missing_data_df['missing_percentage'], color='coral')\n",
    "ax1.set_title('Missing/Corrupted Data Percentage by Category')\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Percentage (%)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(missing_data_df['missing_percentage']):\n",
    "    ax1.text(i, v + 0.1, f'{v:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# 2. File size distribution\n",
    "ax2.bar(missing_data_df['category'], missing_data_df['avg_file_size_mb'], color='lightblue')\n",
    "ax2.set_title('Average File Size by Category')\n",
    "ax2.set_xlabel('Category')\n",
    "ax2.set_ylabel('File Size (MB)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(missing_data_df['avg_file_size_mb']):\n",
    "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Image dimensions comparison\n",
    "if not dimension_df.empty:\n",
    "    categories_dim = dimension_df['category']\n",
    "    x_pos = np.arange(len(categories_dim))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x_pos - width/2, dimension_df['avg_width'], width, label='Width', color='skyblue')\n",
    "    ax3.bar(x_pos + width/2, dimension_df['avg_height'], width, label='Height', color='lightcoral')\n",
    "    ax3.set_title('Average Image Dimensions by Category')\n",
    "    ax3.set_xlabel('Category')\n",
    "    ax3.set_ylabel('Pixels')\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels(categories_dim, rotation=45)\n",
    "    ax3.legend()\n",
    "\n",
    "# 4. Data quality summary\n",
    "readable_images = missing_data_df['readable_images'].sum()\n",
    "corrupted_images = missing_data_df['corrupted_images'].sum()\n",
    "total_analyzed = readable_images + corrupted_images\n",
    "\n",
    "ax4.pie([readable_images, corrupted_images], \n",
    "        labels=['Readable Images', 'Corrupted Images'], \n",
    "        autopct='%1.2f%%', \n",
    "        colors=['lightgreen', 'lightcoral'],\n",
    "        startangle=90)\n",
    "ax4.set_title('Overall Data Quality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 1: DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images analyzed: {total_analyzed}\")\n",
    "print(f\"Readable images: {readable_images} ({(readable_images/total_analyzed*100):.2f}%)\")\n",
    "print(f\"Corrupted/Missing images: {corrupted_images} ({(corrupted_images/total_analyzed*100):.2f}%)\")\n",
    "print(f\"Overall data quality: {(readable_images/total_analyzed*100):.2f}% good\")\n",
    "print(\"\\nCategory-wise missing data:\")\n",
    "for _, row in missing_data_df.iterrows():\n",
    "    print(f\"  - {row['category']}: {row['missing_percentage']:.2f}% missing/corrupted\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff335457",
   "metadata": {},
   "source": [
    "## Task 1 Findings and Observations\n",
    "\n",
    "### Key Findings:\n",
    "1. **Dataset Composition**: The dataset contains 4 categories of eye disease images\n",
    "2. **Missing Data Analysis**: Systematic check for corrupted or unreadable image files\n",
    "3. **File Quality**: Assessment of image file integrity and readability\n",
    "4. **Dimension Analysis**: Understanding of image size variations across categories\n",
    "\n",
    "### Observations:\n",
    "- **Data Quality**: [Results will show percentage of corrupted/missing images]\n",
    "- **File Size Consistency**: [Results will show if file sizes are consistent across categories]\n",
    "- **Image Dimensions**: [Results will show if images have consistent dimensions]\n",
    "- **Category Balance**: [Results will show if categories are balanced]\n",
    "\n",
    "### Next Steps for Task 2:\n",
    "Based on the missing data analysis, we can now:\n",
    "1. **Identify** the specific missing data handling method needed\n",
    "2. **Choose** appropriate strategies (removal, interpolation, or data augmentation)\n",
    "3. **Justify** the chosen method based on the percentage and type of missing data found\n",
    "\n",
    "### Data Quality Recommendations:\n",
    "- If corrupted images < 5%: Remove corrupted files\n",
    "- If corrupted images 5-15%: Consider data augmentation to balance\n",
    "- If corrupted images > 15%: Investigate data collection process\n",
    "\n",
    "**Ready for Task 2: Choose and justify missing data handling method** ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974d054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
